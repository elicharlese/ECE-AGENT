{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGENT LLM Fine-tuning Pipeline\n",
    "\n",
    "This notebook fine-tunes the locally downloaded Llama models using user interaction data collected from the deployed AGENT system.\n",
    "\n",
    "## Features:\n",
    "- **Zero-cost training** using Google Colab free GPU\n",
    "- **LoRA fine-tuning** for efficient parameter updates\n",
    "- **Multi-agent specialization** training\n",
    "- **Quality filtering** based on user feedback\n",
    "- **Automated evaluation** and model comparison\n",
    "\n",
    "## Requirements:\n",
    "- Upload your training data JSON file\n",
    "- Ensure you have the base models downloaded locally\n",
    "- Free Google Colab GPU runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q accelerate peft bitsandbytes transformers trl datasets evaluate\n",
    "!pip install -q sentence-transformers chromadb\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "# Mount Google Drive for model storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p /content/models\n",
    "!mkdir -p /content/training_data\n",
    "!mkdir -p /content/fine_tuned_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload your training data JSON file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load training data\n",
    "training_data_path = list(uploaded.keys())[0]\n",
    "print(f\"Loading training data from: {training_data_path}\")\n",
    "\n",
    "with open(training_data_path, 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(training_data['examples'])} training examples\")\n",
    "print(f\"Agent modes: {training_data['metadata']['agent_modes']}\")\n",
    "\n",
    "# Display data summary\n",
    "df = pd.DataFrame(training_data['examples'])\n",
    "print(\"\\nData Summary:\")\n",
    "print(df.groupby('agent_mode').agg({\n",
    "    'quality_score': ['count', 'mean', 'std'],\n",
    "    'reasoning_steps': 'mean'\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama3.1-8b\": {\n",
    "        \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \"max_seq_length\": 2048,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.05\n",
    "    },\n",
    "    \"codellama-7b\": {\n",
    "        \"model_name\": \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "        \"max_seq_length\": 2048,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.05\n",
    "    },\n",
    "    \"mistral-7b\": {\n",
    "        \"model_name\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        \"max_seq_length\": 2048,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.05\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select model to fine-tune\n",
    "SELECTED_MODEL = \"llama3.1-8b\"  # Change this to train different models\n",
    "config = MODEL_CONFIGS[SELECTED_MODEL]\n",
    "\n",
    "print(f\"Selected model: {SELECTED_MODEL}\")\n",
    "print(f\"Model config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare model for training\n",
    "def load_model_for_training(model_name, config):\n",
    "    \"\"\"Load model with 4-bit quantization for efficient training.\"\"\"\n",
    "    \n",
    "    # 4-bit quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Add padding token if missing\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading {SELECTED_MODEL} model...\")\n",
    "model, tokenizer = load_model_for_training(config[\"model_name\"], config)\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=config[\"lora_r\"],\n",
    "    lora_alpha=config[\"lora_alpha\"],\n",
    "    lora_dropout=config[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(f\"Model loaded and configured for training!\")\n",
    "print(f\"Trainable parameters: {model.get_nb_trainable_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training dataset\n",
    "def prepare_training_dataset(examples, tokenizer, max_length=2048):\n",
    "    \"\"\"Prepare dataset for instruction tuning.\"\"\"\n",
    "    \n",
    "    formatted_data = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Format as instruction-response pair\n",
    "        instruction = example[\"instruction\"]\n",
    "        input_text = example[\"input\"]\n",
    "        output_text = example[\"output\"]\n",
    "        \n",
    "        # Create prompt in chat format\n",
    "        if \"llama\" in SELECTED_MODEL.lower():\n",
    "            # Llama chat format\n",
    "            prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{output_text}<|eot_id|>\"\n",
    "        elif \"mistral\" in SELECTED_MODEL.lower():\n",
    "            # Mistral chat format\n",
    "            prompt = f\"<s>[INST] {instruction}\\n\\n{input_text} [/INST] {output_text}</s>\"\n",
    "        else:\n",
    "            # Generic format\n",
    "            prompt = f\"System: {instruction}\\n\\nUser: {input_text}\\n\\nAssistant: {output_text}\"\n",
    "        \n",
    "        formatted_data.append({\"text\": prompt, \"quality_score\": example[\"quality_score\"]})\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_list(formatted_data)\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "# Prepare dataset\n",
    "print(\"Preparing training dataset...\")\n",
    "train_dataset = prepare_training_dataset(\n",
    "    training_data[\"examples\"], \n",
    "    tokenizer, \n",
    "    max_length=config[\"max_seq_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Training dataset prepared with {len(train_dataset)} examples\")\n",
    "\n",
    "# Split into train/validation\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "val_dataset = train_val_split[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} examples\")\n",
    "print(f\"Validation: {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"/content/fine_tuned_models/{SELECTED_MODEL}\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\"  # Disable wandb/tensorboard for simplicity\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Not using masked language modeling\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"- Model: {SELECTED_MODEL}\")\n",
    "print(f\"- Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"- Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"- Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"- Training steps: ~{len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"🚀 Starting fine-tuning...\")\n",
    "print(\"This may take 1-2 hours depending on your dataset size\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "train_result = trainer.train()\n",
    "end_time = datetime.now()\n",
    "\n",
    "training_duration = end_time - start_time\n",
    "print(f\"\\n✅ Training completed in {training_duration}\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_save_path = f\"/content/fine_tuned_models/{SELECTED_MODEL}_agent_finetuned\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"\\n💾 Model saved to: {model_save_path}\")\n",
    "\n",
    "# Copy to Google Drive for persistence\n",
    "drive_save_path = f\"/content/drive/MyDrive/agent_models/{SELECTED_MODEL}_agent_finetuned\"\n",
    "!mkdir -p \"/content/drive/MyDrive/agent_models\"\n",
    "!cp -r \"{model_save_path}\" \"{drive_save_path}\"\n",
    "\n",
    "print(f\"📁 Model also saved to Google Drive: {drive_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "def evaluate_model(model, tokenizer, test_examples):\n",
    "    \"\"\"Evaluate model performance on test examples.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for i, example in enumerate(test_examples[:10]):  # Test first 10 examples\n",
    "        instruction = example[\"instruction\"]\n",
    "        input_text = example[\"input\"]\n",
    "        expected_output = example[\"output\"]\n",
    "        \n",
    "        # Create prompt\n",
    "        if \"llama\" in SELECTED_MODEL.lower():\n",
    "            prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        elif \"mistral\" in SELECTED_MODEL.lower():\n",
    "            prompt = f\"<s>[INST] {instruction}\\n\\n{input_text} [/INST] \"\n",
    "        else:\n",
    "            prompt = f\"System: {instruction}\\n\\nUser: {input_text}\\n\\nAssistant: \"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the assistant's response\n",
    "        if \"assistant\" in generated_text.lower():\n",
    "            response = generated_text.split(\"assistant:\", 1)[-1].strip()\n",
    "        else:\n",
    "            response = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        results.append({\n",
    "            \"input\": input_text,\n",
    "            \"expected\": expected_output,\n",
    "            \"generated\": response,\n",
    "            \"agent_mode\": example[\"agent_mode\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n--- Test Example {i+1} ({example['agent_mode']}) ---\")\n",
    "        print(f\"Input: {input_text[:100]}...\")\n",
    "        print(f\"Expected: {expected_output[:100]}...\")\n",
    "        print(f\"Generated: {response[:100]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "print(\"🔍 Evaluating fine-tuned model...\")\n",
    "eval_results = evaluate_model(model, tokenizer, training_data[\"examples\"])\n",
    "\n",
    "# Save evaluation results\n",
    "eval_save_path = f\"/content/fine_tuned_models/{SELECTED_MODEL}_evaluation.json\"\n",
    "with open(eval_save_path, 'w') as f:\n",
    "    json.dump({\n",
    "        \"model\": SELECTED_MODEL,\n",
    "        \"training_duration\": str(training_duration),\n",
    "        \"final_loss\": train_result.training_loss,\n",
    "        \"evaluation_results\": eval_results,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\n📊 Evaluation results saved to: {eval_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment package\n",
    "def create_deployment_package(model_path, model_name):\n",
    "    \"\"\"Create a deployment package for the fine-tuned model.\"\"\"\n",
    "    \n",
    "    import shutil\n",
    "    \n",
    "    # Create deployment directory\n",
    "    deploy_dir = f\"/content/deployment/{model_name}\"\n",
    "    os.makedirs(deploy_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy model files\n",
    "    shutil.copytree(model_path, f\"{deploy_dir}/model\")\n",
    "    \n",
    "    # Create model info\n",
    "    model_info = {\n",
    "        \"model_name\": model_name,\n",
    "        \"base_model\": config[\"model_name\"],\n",
    "        \"fine_tuned_date\": datetime.now().isoformat(),\n",
    "        \"training_data_size\": len(training_data[\"examples\"]),\n",
    "        \"agent_modes\": training_data[\"metadata\"][\"agent_modes\"],\n",
    "        \"lora_config\": {\n",
    "            \"r\": config[\"lora_r\"],\n",
    "            \"alpha\": config[\"lora_alpha\"],\n",
    "            \"dropout\": config[\"lora_dropout\"]\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"final_loss\": train_result.training_loss,\n",
    "            \"training_duration\": str(training_duration)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{deploy_dir}/model_info.json\", 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    \n",
    "    # Create deployment script\n",
    "    deploy_script = f\"\"\"\n",
    "#!/bin/bash\n",
    "# AGENT Model Deployment Script\n",
    "\n",
    "MODEL_NAME=\"{model_name}\"\n",
    "MODEL_PATH=\"./model\"\n",
    "\n",
    "# Copy to Ollama models directory\n",
    "cp -r \"$MODEL_PATH\" \"/usr/local/lib/ollama/models/$MODEL_NAME\"\n",
    "\n",
    "# Update Ollama configuration\n",
    "echo \"Model $MODEL_NAME deployed successfully!\"\n",
    "echo \"Run: ollama run $MODEL_NAME\"\n",
    "\"\"\"\n",
    "    \n",
    "    with open(f\"{deploy_dir}/deploy.sh\", 'w') as f:\n",
    "        f.write(deploy_script)\n",
    "    \n",
    "    # Make script executable\n",
    "    os.chmod(f\"{deploy_dir}/deploy.sh\", 0o755)\n",
    "    \n",
    "    # Create zip archive\n",
    "    zip_path = f\"/content/{model_name}_deployment.zip\"\n",
    "    shutil.make_archive(f\"/content/{model_name}_deployment\", 'zip', deploy_dir)\n",
    "    \n",
    "    print(f\"📦 Deployment package created: {zip_path}\")\n",
    "    print(f\"📁 Deployment directory: {deploy_dir}\")\n",
    "    \n",
    "    return zip_path, deploy_dir\n",
    "\n",
    "# Create deployment package\n",
    "print(\"📦 Creating deployment package...\")\n",
    "zip_path, deploy_dir = create_deployment_package(model_save_path, f\"{SELECTED_MODEL}_agent\")\n",
    "\n",
    "# Download deployment package\n",
    "from google.colab import files\n",
    "files.download(zip_path)\n",
    "\n",
    "print(\"\\n🎉 Fine-tuning complete!\")\n",
    "print(f\"✅ Model: {SELECTED_MODEL}\")\n",
    "print(f\"✅ Training Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"✅ Training Time: {training_duration}\")\n",
    "print(f\"✅ Deployment Package: {zip_path}\")\n",
    "print(\"\\n📋 Next Steps:\")\n",
    "print(\"1. Download the deployment package\")\n",
    "print(\"2. Extract and run deploy.sh on your local machine\")\n",
    "print(\"3. Update your AGENT system to use the new model\")\n",
    "print(\"4. Test the improved responses!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
